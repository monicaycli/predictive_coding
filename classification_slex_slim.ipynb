{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default parameters\n",
    "\n",
    "param = {'USE_MASK': False,\n",
    "         'GAUSS_MASK_SIGMA': 1.0,\n",
    "         'IMAGE_FILTER': (-1,1),\n",
    "         'DOG_KSIZE': (5,5),\n",
    "         'DOG_SIGMA1': 1.3,\n",
    "         'DOG_SIGMA2': 2.6,\n",
    "         'INPUT_SCALE': 1.0,\n",
    "         'ITER_N': 1,\n",
    "         'EPOCH_N': 100,\n",
    "         'CLEAR_SAVED_WEIGHTS': True,\n",
    "         'IN_DIR': 'data/slex_len3_small',\n",
    "         'OUT_DIR': 'results/slex_len3_small_results',\n",
    "         'RF1_SIZE': {'x': 1, 'y': 3},\n",
    "         'RF1_OFFSET': {'x': 1, 'y': 3},\n",
    "         'RF1_LAYOUT': {'x': 1, 'y': 7},\n",
    "         'LEVEL1_MODULE_SIZE': 8,\n",
    "         'LEVEL2_MODULE_SIZE': 32,\n",
    "         'ALPHA_R': 0.1,\n",
    "         'ALPHA_U': 0.1,\n",
    "         'ALPHA_V': 0.1,\n",
    "         'ALPHA_DECAY': 1,\n",
    "         'ALPHA_MIN': 0,\n",
    "         'TEST_INTERVAL': 10,\n",
    "         'AF': 'linear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from dataset import Dataset\n",
    "from model import Model\n",
    "\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import shutil\n",
    "from itertools import product\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIT_COMMIT_HASH = os.popen('git rev-parse --short HEAD').read().replace('\\n', '')\n",
    "print(GIT_COMMIT_HASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start:', datetime.now(pytz.timezone('US/Eastern')).strftime('%c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Paramters and Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = pd.Series(param)\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "in_dir = param.IN_DIR\n",
    "out_dir = param.OUT_DIR\n",
    "\n",
    "test_set = Dataset(scale=param.INPUT_SCALE,\n",
    "                   shuffle=False,\n",
    "                   data_dir=in_dir,\n",
    "                   rf1_x=param.RF1_SIZE['x'],\n",
    "                   rf1_y=param.RF1_SIZE['y'],\n",
    "                   rf1_offset_x=param.RF1_OFFSET['x'],\n",
    "                   rf1_offset_y=param.RF1_OFFSET['y'],\n",
    "                   rf1_layout_x=param.RF1_LAYOUT['x'],\n",
    "                   rf1_layout_y=param.RF1_LAYOUT['y'],\n",
    "                   use_mask=param.USE_MASK,\n",
    "                   gauss_mask_sigma=param.GAUSS_MASK_SIGMA,\n",
    "                   image_filter=param.IMAGE_FILTER,\n",
    "                   DoG_ksize=param.DOG_KSIZE,\n",
    "                   DoG_sigma1=param.DOG_SIGMA1,\n",
    "                   DoG_sigma2=param.DOG_SIGMA2)\n",
    "\n",
    "test_int = param.TEST_INTERVAL\n",
    "\n",
    "epoch_n = param.EPOCH_N\n",
    "zero_pad_len = len(str(epoch_n))\n",
    "\n",
    "model = Model(dataset=test_set,\n",
    "              level1_module_size=param.LEVEL1_MODULE_SIZE,\n",
    "              level2_module_size=param.LEVEL2_MODULE_SIZE)\n",
    "\n",
    "# parameters\n",
    "model.iteration = param.ITER_N\n",
    "\n",
    "model.alpha_r = param.ALPHA_R\n",
    "model.alpha_u = param.ALPHA_U\n",
    "model.alpha_v = param.ALPHA_V\n",
    "\n",
    "model.af = param.AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "if param.CLEAR_SAVED_WEIGHTS == True and os.path.exists(out_dir):\n",
    "    shutil.rmtree(out_dir)\n",
    "\n",
    "# determining which set of weights to use\n",
    "\n",
    "out_dir_all = glob.glob(os.path.join(out_dir, '*'))\n",
    "out_dir_epoch = glob.glob(os.path.join(out_dir, 'epoch_*'))\n",
    "out_dir_pretrain = os.path.join(out_dir, 'pretraining')\n",
    "\n",
    "if len(out_dir_epoch) > 0:\n",
    "    # load weights from previous results\n",
    "    regex = re.compile(os.path.join(out_dir, 'epoch_(?P<epoch>.*)'))\n",
    "    epoch_all = [int(regex.match(x).group('epoch')) for x in out_dir_epoch]\n",
    "    epoch_max_idx = np.argmax(epoch_all)\n",
    "    epoch_max = epoch_all[epoch_max_idx]\n",
    "    model.load(out_dir_epoch[epoch_max_idx])\n",
    "elif out_dir_pretrain not in out_dir_all:\n",
    "    # save current pretraining weights\n",
    "    epoch_max = -1\n",
    "    model.save(out_dir_pretrain)\n",
    "else:\n",
    "    # load previous pretraining weights\n",
    "    epoch_max = -1\n",
    "    model.save(out_dir_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay over epochs\n",
    "u = np.array([param.ALPHA_U/(param.ALPHA_DECAY**i) for i in range(param.EPOCH_N)])\n",
    "v = np.array([param.ALPHA_V/(param.ALPHA_DECAY**i) for i in range(param.EPOCH_N)])\n",
    "u[u < param.ALPHA_MIN] = param.ALPHA_MIN\n",
    "v[v < param.ALPHA_MIN] = param.ALPHA_MIN\n",
    "plt.plot(u, label='u');\n",
    "plt.plot(v, label='v');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "for epoch in (x for x in range(epoch_n) if x > epoch_max):\n",
    "    # learning rate\n",
    "    model.alpha_u = u[epoch]\n",
    "    model.alpha_v = v[epoch]\n",
    "    \n",
    "    # images are shuffled for each training epoch\n",
    "    train_set = Dataset(scale=param.INPUT_SCALE,\n",
    "                        shuffle=True,\n",
    "                        data_dir=in_dir,\n",
    "                        rf1_x=param.RF1_SIZE['x'],\n",
    "                        rf1_y=param.RF1_SIZE['y'],\n",
    "                        rf1_offset_x=param.RF1_OFFSET['x'],\n",
    "                        rf1_offset_y=param.RF1_OFFSET['y'],\n",
    "                        rf1_layout_x=param.RF1_LAYOUT['x'],\n",
    "                        rf1_layout_y=param.RF1_LAYOUT['y'],\n",
    "                        use_mask=param.USE_MASK,\n",
    "                        gauss_mask_sigma=param.GAUSS_MASK_SIGMA,\n",
    "                        image_filter=param.IMAGE_FILTER,\n",
    "                        DoG_ksize=param.DOG_KSIZE,\n",
    "                        DoG_sigma1=param.DOG_SIGMA1,\n",
    "                        DoG_sigma2=param.DOG_SIGMA2)\n",
    "\n",
    "    # replaced model.train(train_set)\n",
    "    for word_i in range(train_set.rf2_patches.shape[0]):\n",
    "        inputs = train_set.rf2_patches[word_i]\n",
    "        labels = train_set.labels[word_i]\n",
    "\n",
    "        # HACK: remove rf2 patches where all values are identical (assuming no variation = silence or no information)\n",
    "        bool_mask = np.max(inputs, axis=(2,3)) != np.min(inputs, axis=(2,3))\n",
    "        mask_y = bool_mask.any(axis=1).sum()\n",
    "        mask_x = bool_mask.any(axis=0).sum()\n",
    "        \n",
    "        inputs = inputs[bool_mask].reshape((mask_y, mask_x) + inputs.shape[2:])\n",
    "        labels = labels[bool_mask].reshape((mask_y, mask_x) + labels.shape[2:])\n",
    "\n",
    "        output = pd.DataFrame.from_dict(model.apply_input(inputs, labels, train_set, training=True))\n",
    "\n",
    "    if epoch == 0 or epoch % test_int == test_int-1:\n",
    "        model.save(os.path.join(out_dir, 'epoch_{:0>{}d}'.format(epoch, zero_pad_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(glob.glob(os.path.join(in_dir, '*.png')))\n",
    "\n",
    "regex = re.compile(os.path.join(in_dir, '(?P<index>.*)_(?P<word>.*).png'))\n",
    "f_index = [regex.match(x).group('index') for x in filenames]\n",
    "f_word = [regex.match(x).group('word') for x in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # randomly choose 10 words to plot\n",
    "# word_select = sorted(np.random.choice(len(test_set.filtered_images), 10, replace=False))\n",
    "\n",
    "# or select all words\n",
    "word_select = list(range(len(test_set.filtered_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Simulation Results as a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data by epoch for parallelization\n",
    "\n",
    "def load_data(epoch, iteration):\n",
    "    \n",
    "    results = {'epoch': [], 'target': [], 'idx': [], 'last_idx': [], 'iteration': [],\n",
    "               'node': [], 'node_word': [], 'target_label': [],\n",
    "               'activation_raw': [], 'activation': [],\n",
    "               'target_n': [], 'response_n': [], 'accuracy': [],\n",
    "               'e10': [], 'e21': [], 'e32': [], 'e43': [], 'e11': [], 'e22': [], 'e33': [], 'e_all': []}\n",
    "\n",
    "    filenames = sorted(glob.glob(os.path.join(out_dir, 'epoch_*')))\n",
    "    regex = re.compile(os.path.join(out_dir, 'epoch_(?P<pad>0*)(?P<epoch>.+)'))\n",
    "    f_epoch = [int(regex.match(x).group('epoch')) for x in filenames]\n",
    "    \n",
    "    model.load(filenames[f_epoch.index(epoch)])\n",
    "    \n",
    "    for word_i in np.ndindex(test_set.rf2_patches.shape[0]):\n",
    "\n",
    "        inputs = test_set.rf2_patches[word_i]\n",
    "        labels = test_set.labels[word_i]\n",
    "\n",
    "        # HACK: remove rf2 patches where all values are identical (assuming no variation = silence or no information)\n",
    "        bool_mask = np.max(inputs, axis=(2,3)) != np.min(inputs, axis=(2,3))\n",
    "        mask_y = bool_mask.any(axis=1).sum()\n",
    "        mask_x = bool_mask.any(axis=0).sum()\n",
    "        inputs = inputs[bool_mask].reshape((mask_y, mask_x) + inputs.shape[2:])\n",
    "        labels = labels[bool_mask].reshape((mask_y, mask_x) + labels.shape[2:])\n",
    "\n",
    "        output = pd.DataFrame.from_dict(model.apply_input(inputs, labels, test_set, training=False))\n",
    "        \n",
    "        idx_list = [x for x in output.index if output.iteration[x] == iteration]\n",
    "        \n",
    "        for idx in idx_list:\n",
    "            last_idx = True if idx == max(idx_list) else False\n",
    "        \n",
    "            target_n = np.argmax(output.label[idx])\n",
    "\n",
    "            r3_raw = output.r3[idx].astype(np.float128)\n",
    "            r3 = np.exp(r3_raw)/np.sum(np.exp(r3_raw))\n",
    "\n",
    "            if sum(r3 == r3.max()) != 1:\n",
    "                response_n = None\n",
    "            else:\n",
    "                response_n = np.argmax(r3)\n",
    "\n",
    "            if target_n == response_n:\n",
    "                accuracy = 1\n",
    "            else:\n",
    "                accuracy = 0\n",
    "                \n",
    "            e10 = output.e10[idx].flatten().T @ output.e10[idx].flatten()\n",
    "            e21 = output.e21[idx].flatten().T @ output.e21[idx].flatten()\n",
    "            e32 = output.e32[idx].flatten().T @ output.e32[idx].flatten()\n",
    "            e43 = output.e43[idx].flatten().T @ output.e43[idx].flatten()\n",
    "            e11 = output.e11[idx].flatten().T @ output.e11[idx].flatten()\n",
    "            e22 = output.e22[idx].flatten().T @ output.e22[idx].flatten()\n",
    "            e33 = output.e33[idx].flatten().T @ output.e33[idx].flatten()\n",
    "            e_all = e10 + e21 + e32 + e43 + e11 + e22 + e33\n",
    "\n",
    "            results['epoch'] += [epoch] * len(r3)\n",
    "            results['target'] += [f_word[target_n]] * len(r3)\n",
    "            results['idx'] += [idx] * len(r3)\n",
    "            results['last_idx'] += [last_idx] * len(r3)\n",
    "            results['iteration'] += [output.iteration[idx]] * len(r3)\n",
    "            results['node'] += list(range(len(r3)))\n",
    "            results['node_word'] += [f_word[x] for x in list(range(len(r3)))]\n",
    "            results['target_label'] += list(output.label[idx])\n",
    "            results['activation_raw'] += list(r3_raw)\n",
    "            results['activation'] += list(r3)\n",
    "            results['target_n'] += [target_n] * len(r3)\n",
    "            results['response_n'] += [response_n] * len(r3)\n",
    "            results['accuracy'] += [accuracy] * len(r3)\n",
    "            \n",
    "            results['e10'] += [e10] * len(r3)\n",
    "            results['e21'] += [e21] * len(r3)\n",
    "            results['e32'] += [e32] * len(r3)\n",
    "            results['e43'] += [e43] * len(r3)\n",
    "            results['e11'] += [e11] * len(r3)\n",
    "            results['e22'] += [e22] * len(r3)\n",
    "            results['e33'] += [e33] * len(r3)\n",
    "            results['e_all'] += [e_all] * len(r3)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "pool = mp.Pool(8) # may increase number of CPU if available\n",
    "\n",
    "epoch_list = (x for x in range(epoch_n) if x == 0 or x % test_int == test_int-1)\n",
    "iter_list = (x for x in range(model.iteration) if x == max(range(model.iteration)))\n",
    "\n",
    "df_list = pool.starmap(load_data, product(epoch_list, iter_list))\n",
    "pool.close()\n",
    "\n",
    "results_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_given_target(target, word):\n",
    "\n",
    "    if target == word:\n",
    "        category = 'target'\n",
    "    elif target[0:2] == word[0:2]:\n",
    "        category = 'cohort'\n",
    "    elif target[1:] == word[1:]:\n",
    "        category = 'rhyme'\n",
    "    elif word in target:\n",
    "        category = 'embedded'\n",
    "    else:\n",
    "        category = 'other'\n",
    "\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_combo_df = pd.DataFrame(list(product(f_word, f_word)), columns=['target','node_word'])\n",
    "\n",
    "word_combo_df['category'] = word_combo_df.apply(lambda x: category_given_target(x['target'], x['node_word']), axis = 1)\n",
    "cat_list = ['target', 'cohort', 'rhyme', 'other', 'embedded']\n",
    "word_combo_df.category = word_combo_df.category.astype('category').cat.set_categories(cat_list)\n",
    "\n",
    "results_df = pd.merge(results_df, word_combo_df, on=['target', 'node_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to by-target accuracy, as opposed to by-idx\n",
    "accuracy_by_target = results_df[(results_df.last_idx == True)].groupby(['epoch','target_n'])['accuracy'].mean()\n",
    "results_df = pd.merge(results_df, accuracy_by_target, on=['epoch','target_n'], suffixes=('_idx', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle(os.path.join(out_dir, 'results.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_pickle(os.path.join(out_dir, 'results.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Change by Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "ALL_WEIGHTS = {\"epoch\": [], \"U1\": [], \"U2\": [], \"U3\": [], \"V1\": [], \"V2\": [], \"V3\": []}\n",
    "\n",
    "model.load(out_dir_pretrain)\n",
    "\n",
    "ALL_WEIGHTS[\"epoch\"].append(-1)\n",
    "ALL_WEIGHTS[\"U1\"].append(model.U1.mean())\n",
    "ALL_WEIGHTS[\"U2\"].append(model.U2.mean())\n",
    "ALL_WEIGHTS[\"U3\"].append(model.U3.mean())\n",
    "ALL_WEIGHTS[\"V1\"].append(model.V1.mean())\n",
    "ALL_WEIGHTS[\"V2\"].append(model.V2.mean())\n",
    "ALL_WEIGHTS[\"V3\"].append(model.V3.mean())\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    if epoch % test_int == test_int-1:\n",
    "        filenames = sorted(glob.glob(os.path.join(out_dir, 'epoch_*')))\n",
    "        regex = re.compile(os.path.join(out_dir, 'epoch_(?P<pad>0*)(?P<epoch>.+)'))\n",
    "        f_epoch = [int(regex.match(x).group('epoch')) for x in filenames]\n",
    "\n",
    "        model.load(filenames[f_epoch.index(epoch)])\n",
    "        \n",
    "        ALL_WEIGHTS[\"epoch\"].append(epoch)\n",
    "        ALL_WEIGHTS[\"U1\"].append(model.U1.mean())\n",
    "        ALL_WEIGHTS[\"U2\"].append(model.U2.mean())\n",
    "        ALL_WEIGHTS[\"U3\"].append(model.U3.mean())\n",
    "        ALL_WEIGHTS[\"V1\"].append(model.V1.mean())\n",
    "        ALL_WEIGHTS[\"V2\"].append(model.V2.mean())\n",
    "        ALL_WEIGHTS[\"V3\"].append(model.V3.mean())\n",
    "        \n",
    "ALL_WEIGHTS_DF = pd.DataFrame.from_dict(ALL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WEIGHTS_DF.groupby('epoch').mean().plot();\n",
    "plt.legend(loc='lower left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss by Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby(['epoch'])[['e10','e21','e32','e43','e11','e22','e33','e_all']].mean().plot();\n",
    "plt.legend(loc='lower left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy by Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[(results_df.last_idx == True)].groupby('epoch')['accuracy'].mean().plot(ylim=(0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy by Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df[(results_df.last_idx == True)].groupby('target')['accuracy'].mean().plot.bar(figsize=(40,5), ylim=(0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation of Top 10 Activated Items for a Given Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_cutoff = int(epoch_n * 1/5)\n",
    "epoch_cutoff = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots settings\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(len(word_select)/ncols))\n",
    "subplot_x, subplot_y = (5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows, ncols, figsize=(subplot_x*ncols, subplot_y*nrows), sharey=True)\n",
    "\n",
    "for word_i in word_select:\n",
    "    results_df_select = results_df[(results_df.target_n == word_i) & (results_df.epoch >= epoch_cutoff) & (results_df.last_idx == True)]\n",
    "    top_10 = results_df_select[(results_df_select.last_idx == True) & (results_df_select.epoch == max(results_df_select.epoch))].sort_values(by=['activation'], ascending=False).node_word[0:10]\n",
    "    results_df_select = results_df_select.loc[results_df_select.node_word.isin(top_10)]\n",
    "    \n",
    "    # order based on average activation values\n",
    "    results_df_select.node_word = results_df_select.node_word.astype('category').cat.set_categories(top_10)\n",
    "    \n",
    "    # plot\n",
    "    df_select = results_df_select.groupby(['epoch','node_word']).mean()['activation'].unstack()\n",
    "    df_plot = df_select.plot(title='target: {}'.format(f_word[word_i]), ax=axes[word_select.index(word_i)//ncols, word_select.index(word_i)%ncols]);\n",
    "    \n",
    "    # thicken target line\n",
    "    lws = results_df_select.groupby('node_word').mean()['target_label']*2+1\n",
    "    \n",
    "    for i, l in enumerate(df_plot.lines):\n",
    "        plt.setp(l, linewidth=lws[i])\n",
    "        \n",
    "    df_plot.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Timesteps at the Last Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows, ncols, figsize=(subplot_x*ncols, subplot_y*nrows), sharey=True)\n",
    "\n",
    "for word_i in word_select:\n",
    "    results_df_select = results_df[(results_df.target_n == word_i) & (results_df.epoch == max(results_df.epoch))]\n",
    "    top_10 = results_df_select[(results_df_select.last_idx == True) & (results_df_select.epoch == max(results_df_select.epoch))].sort_values(by=['activation'], ascending=False).node_word[0:10]\n",
    "    results_df_select = results_df_select[results_df_select.node_word.isin(top_10)]\n",
    "    \n",
    "    # order based on average activation values\n",
    "    results_df_select.node_word = results_df_select.node_word.astype('category').cat.set_categories(top_10)\n",
    "\n",
    "    # plot\n",
    "    df_select = results_df_select.groupby(['idx','node_word']).mean()['activation'].unstack()\n",
    "    df_plot = df_select.plot(title='target: {}'.format(f_word[word_i]), ax=axes[word_select.index(word_i)//ncols, word_select.index(word_i)%ncols]);\n",
    "    \n",
    "    # thicken target line\n",
    "    lws = results_df_select.groupby('node_word').mean()['target_label']*2+1\n",
    "    \n",
    "    for i, l in enumerate(df_plot.lines):\n",
    "        plt.setp(l, linewidth=lws[i])\n",
    "        \n",
    "    df_plot.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation by Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Across All Items Over Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"epoch\", y=\"activation\", hue=\"category\", hue_order=cat_list, err_style=None,\n",
    "             data=results_df[(results_df.last_idx == True) & (results_df.epoch >= epoch_cutoff)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"epoch\", y=\"activation\", hue=\"category\", hue_order=cat_list, err_style=None,\n",
    "             data=results_df[(results_df.last_idx == True) & (results_df.epoch >= epoch_cutoff) & (results_df.accuracy == 1)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"epoch\", y=\"activation\", hue=\"category\", hue_order=cat_list, err_style=None,\n",
    "             data=results_df[(results_df.last_idx == True) & (results_df.epoch >= epoch_cutoff) & (results_df.accuracy == 0)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Across All Items Over Timesteps at the Last Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"idx\", y=\"activation\", hue=\"category\", hue_order=cat_list, err_style=\"band\",\n",
    "             data=results_df[(results_df.epoch == max(range(epoch_n)))]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"idx\", y=\"activation\", hue=\"category\", hue_order=cat_list, err_style=\"band\",\n",
    "             data=results_df[(results_df.epoch == max(range(epoch_n))) & (results_df.accuracy == 1)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"idx\", y=\"activation\", hue=\"category\", hue_order=cat_list, err_style=\"band\",\n",
    "             data=results_df[(results_df.epoch == max(range(epoch_n))) & (results_df.accuracy == 0)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Item Over Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows, ncols, figsize=(subplot_x*ncols, subplot_y*nrows))\n",
    "\n",
    "for word_i in word_select:\n",
    "    results_df_select = results_df[(results_df.target_n == word_i) & (results_df.epoch >= epoch_cutoff) & (results_df.last_idx == True)]\n",
    "    df_select = results_df_select.groupby(['epoch','category']).mean()['activation'].unstack()\n",
    "    df_plot = df_select.plot(title='target: {}'.format(f_word[word_i]), ax=axes[word_select.index(word_i)//ncols, word_select.index(word_i)%ncols]);\n",
    "    df_plot.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Timesteps at the Last Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows, ncols, figsize=(subplot_x*ncols, subplot_y*nrows))\n",
    "\n",
    "for word_i in word_select:\n",
    "    results_df_select = results_df[(results_df.target_n == word_i) & (results_df.epoch == max(results_df.epoch))]\n",
    "    df_select = results_df_select.groupby(['idx','category']).mean()['activation'].unstack()\n",
    "    df_plot = df_select.plot(title='target: {}'.format(f_word[word_i]), ax=axes[word_select.index(word_i)//ncols, word_select.index(word_i)%ncols]);\n",
    "    df_plot.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationships Between Classification Accuracy and Item Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_acc = results_df.groupby('target')['accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slex_stats = pd.read_csv(os.path.join(in_dir, 'slex.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy by neighborhood density\n",
    "target_N = slex_stats.sort_values(by=['Phono.orig'])['N_sum'].squeeze().values\n",
    "plt.scatter(target_N, target_acc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy by word length\n",
    "target_len = slex_stats.sort_values(by=['Phono.orig'])['n_phon'].squeeze().values\n",
    "plt.scatter(target_len, target_acc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy by Pfreq_sum\n",
    "target_Pfreq_sum = slex_stats.sort_values(by=['Phono.orig'])['Pfreq_sum'].squeeze().values\n",
    "plt.scatter(target_Pfreq_sum, target_acc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('End:', datetime.now(pytz.timezone('US/Eastern')).strftime('%c'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
